# Translator
A custom Transformer encoder–decoder from scratch (no nn.Transformer or high-level attention APIs), implementing multi-head self-attention, masked self-attention, encoder–decoder cross-attention, and sinusoidal positional encodings.
